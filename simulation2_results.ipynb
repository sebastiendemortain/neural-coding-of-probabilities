{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of one voxel encoding for different probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastien/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import useful modules\n",
    "import os\n",
    "import scipy\n",
    "import random as rand\n",
    "from scipy import io as sio\n",
    "from scipy import stats\n",
    "from scipy.stats.stats import pearsonr\n",
    "import numpy as np\n",
    "#import decimal\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')    # To avoid bugs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "width = 18\n",
    "height = 16\n",
    "#matplotlib.rcParams['figure.figsize'] = [width, height]\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import neural_proba\n",
    "from neural_proba import distrib\n",
    "from neural_proba import tuning_curve\n",
    "from neural_proba import voxel\n",
    "from neural_proba import experiment\n",
    "from neural_proba import fmri\n",
    "\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the models\n",
    "\n",
    "Here are the properties related to :\n",
    "- the tuning curves (types, number, variance, ...) both true and fitted. \n",
    "- the neural mixture (sparsities)\n",
    "- the subjects\n",
    "- the sessions\n",
    "- the SNR\n",
    "- the type of linear regression performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d53d55b96275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Load the corresponding data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m [p1_dist_array, p1_mu_array, p1_sd_array] = neural_proba.import_distrib_param(n_subjects, n_sessions, n_stimuli,\n\u001b[0;32m---> 72\u001b[0;31m                                                                                       distrib_type)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Experimental options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master_thesis/neural-coding-of-probabilities/neural_proba.py\u001b[0m in \u001b[0;36mimport_distrib_param\u001b[0;34m(n_subjects, n_sessions, n_stimuli, distrib_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#             p1_sd_array[subject][session] = out_tmp[0, 0].p1_sd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mdata_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct_as_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mp1_mu_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p1_mean_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mp1_sd_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p1_sd_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \"\"\"\n\u001b[1;32m    140\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[0;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mbyte_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mmjv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_matfile_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# All parameters are here\n",
    "\n",
    "# Define the seed to reproduce results from random processes\n",
    "rand.seed(5);\n",
    "\n",
    "# INPUTS\n",
    "\n",
    "# The parameters related to the scheme\n",
    "scheme_array = ['gaussian_ppc', 'sigmoid_ppc', 'gaussian_dpc', 'sigmoid_dpc', 'rate']\n",
    "n_schemes = len(scheme_array)\n",
    "\n",
    "t_mu_gaussian_array = np.array([0.15, 0.1, 7e-2, 5e-2, 4e-2, 3e-2, 2e-2])\n",
    "t_conf_gaussian_array = np.array([0.25, 0.15, 0.10, 8e-2, 6e-2, 4e-2, 3e-2])\n",
    "\n",
    "t_mu_sigmoid_array = np.sqrt(2*np.pi)/4*t_mu_gaussian_array\n",
    "t_conf_sigmoid_array = np.sqrt(2*np.pi)/4*t_conf_gaussian_array\n",
    "\n",
    "N_array = np.array([2, 4, 6, 8, 10, 14, 20])\n",
    "\n",
    "optimal_k_fit_N_array = np.array([2, 1, 2, 1]).astype(int)\n",
    "optimal_fit_N_array = np.zeros_like(optimal_k_fit_N_array).astype(float)\n",
    "optimal_t_mu_array = np.zeros_like(optimal_k_fit_N_array).astype(float)\n",
    "optimal_t_conf_array = np.zeros_like(optimal_k_fit_N_array).astype(float)\n",
    "optimal_t_conf_array = np.zeros_like(optimal_k_fit_N_array).astype(float)\n",
    "\n",
    "for k_fit_scheme in range(n_schemes - 1):    # We exclude rate coding\n",
    "    optimal_fit_N_array[k_fit_scheme] = N_array[optimal_k_fit_N_array[k_fit_scheme]]    # Optimal N\n",
    "    # Now we fill values for optimal mu and t\n",
    "    if k_fit_scheme % 2 == 0:    # Gaussian case\n",
    "        t_mu_tmp = t_mu_gaussian_array[optimal_k_fit_N_array[k_fit_scheme]]\n",
    "        t_conf_tmp = t_conf_gaussian_array[optimal_k_fit_N_array[k_fit_scheme]]\n",
    "        optimal_t_mu_array[k_fit_scheme] = t_mu_tmp\n",
    "        optimal_t_conf_array[k_fit_scheme] = t_conf_tmp\n",
    "    else:\n",
    "        optimal_t_mu_array[k_fit_scheme] = t_mu_sigmoid_array[optimal_k_fit_N_array[k_fit_scheme]]\n",
    "        optimal_t_conf_array[k_fit_scheme] = t_conf_sigmoid_array[optimal_k_fit_N_array[k_fit_scheme]]\n",
    "\n",
    "# Lower and upper bounds of the encoded summary quantity (for tuning curves)\n",
    "tc_lower_bound_mu = 0\n",
    "tc_upper_bound_mu = 1\n",
    "tc_lower_bound_conf = 1.1\n",
    "# we define the upper bound to be a bit away from the highest uncertainty\n",
    "tc_upper_bound_conf = 2.6\n",
    "\n",
    "mu_mean = 0.5    # Mean of the signal of mu's\n",
    "conf_mean = 1.95    # Mean of the signal of conf's\n",
    "\n",
    "# The number of possible N\n",
    "n_N = len(N_array)\n",
    "\n",
    "# The number of fractions tested (related to W)\n",
    "n_fractions = 20# Optimal value : 4000\n",
    "\n",
    "# Sparsity exponents\n",
    "sparsity_exp_array = np.array([1, 2, 4, 8])\n",
    "n_sparsity_exp = len(sparsity_exp_array)\n",
    "\n",
    "# The number of subjects\n",
    "n_subjects = 1000\n",
    "\n",
    "# The number of sessions\n",
    "n_sessions = 4\n",
    "\n",
    "# The number of stimuli per session\n",
    "n_stimuli = 380\n",
    "\n",
    "# Way to compute the distributions from the sequence\n",
    "distrib_type = 'HMM'\n",
    "\n",
    "# Load the corresponding data\n",
    "[p1_dist_array, p1_mu_array, p1_sd_array] = neural_proba.import_distrib_param(n_subjects, n_sessions, n_stimuli,\n",
    "                                                                                      distrib_type)\n",
    "\n",
    "# Experimental options\n",
    "between_stimuli_duration = 1.3\n",
    "min_break_time = 8\n",
    "max_break_time = 12\n",
    "min_n_local_regular_stimuli = 12\n",
    "max_n_local_regular_stimuli = 18\n",
    "\n",
    "# SNR as defined by ||signal||²/(||signal||²+||noise||²)\n",
    "snr = 0.1\n",
    "\n",
    "# fMRI info\n",
    "final_frame_offset = 10  # Frame recording duration after the last stimulus has been shown\n",
    "initial_frame_time = 0\n",
    "dt = 0.125  # Temporal resolution of the fMRI scanner\n",
    "\n",
    "between_scans_duration = 2  # in seconds\n",
    "final_scan_offset = 10  # Scan recording duration after the last stimulus has been shown\n",
    "\n",
    "\n",
    "# Type of regression\n",
    "regr = linear_model.LinearRegression(fit_intercept=True, n_jobs=-1)\n",
    "regr2 = linear_model.LinearRegression(fit_intercept=True, n_jobs=-1)\n",
    "\n",
    "whitening_done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for now\n",
    "n_subjects = 2\n",
    "n_sessions = 4\n",
    "n_N = len(N_array)\n",
    "n_schemes = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X\n",
    "\n",
    "# Initialization of the design matrices and their zscore versions\n",
    "X = [[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fit_scheme in range(n_schemes)]\n",
    "\n",
    "### WE BEGIN BY CREATING THE DESIGN MATRIX X\n",
    "start = time.time()\n",
    "\n",
    "for k_subject in range(n_subjects):\n",
    "    ### Loop over the sessions : we start with it in order to have the same length whatever N_fit is\n",
    "    for k_session in range(n_sessions):\n",
    "        # Get the data of interest\n",
    "        mu = p1_mu_array[k_subject, k_session, :n_stimuli]\n",
    "        sigma = p1_sd_array[k_subject, k_session, :n_stimuli]\n",
    "        conf = -np.log(sigma)\n",
    "        dist = p1_dist_array[k_subject, k_session, :, :n_stimuli]\n",
    "\n",
    "        # Formatting\n",
    "        simulated_distrib = [None for k in range(n_stimuli)]\n",
    "        for k in range(n_stimuli):\n",
    "            # Normalization of the distribution\n",
    "            norm_dist = dist[:, k] * (len(dist[1:, k]) - 1) / np.sum(dist[1:, k])\n",
    "            simulated_distrib[k] = distrib(mu[k], sigma[k], norm_dist)\n",
    "\n",
    "        # Experimental design information\n",
    "        eps = 1e-5  # For floating points issues\n",
    "        initial_time = between_stimuli_duration + eps\n",
    "        final_time_tmp = between_stimuli_duration * (n_stimuli + 1) + eps\n",
    "        # Every 15+-3 trials : one interruption of 8-12s\n",
    "        stimulus_onsets = np.linspace(initial_time, final_time_tmp, n_stimuli)\n",
    "        # We add some time to simulate breaks\n",
    "        stimulus = 0\n",
    "\n",
    "        while True:\n",
    "            # Number of regularly spaced stimuli\n",
    "            n_local_regular_stimuli = rand.randint(min_n_local_regular_stimuli, max_n_local_regular_stimuli)\n",
    "            stimulus_shifted = stimulus + n_local_regular_stimuli  # Current stimulus before the break\n",
    "            if stimulus_shifted > n_stimuli:  # The next break is supposed to occur after all stimuli are shown\n",
    "                break\n",
    "            stimulus_onsets[stimulus_shifted:] += rand.randint(min_break_time,\n",
    "                                                               max_break_time) - between_stimuli_duration  # We consider a break of 8-12s\n",
    "            stimulus = stimulus_shifted\n",
    "\n",
    "        stimulus_durations = dt * np.ones_like(stimulus_onsets)  # Dirac-like stimuli\n",
    "\n",
    "        # fMRI information\n",
    "        final_time = stimulus_onsets[-1]\n",
    "        final_frame_time = final_time + final_frame_offset\n",
    "\n",
    "        initial_scan_time = initial_frame_time + between_scans_duration\n",
    "        final_scan_time = final_time + final_scan_offset\n",
    "        scan_times = np.arange(initial_scan_time, final_scan_time, between_scans_duration)\n",
    "\n",
    "        # Creation of fmri object\n",
    "        simu_fmri = fmri(initial_frame_time, final_frame_time, dt, scan_times)\n",
    "\n",
    "        # Creation of experiment object\n",
    "        exp = experiment(initial_time, final_time, n_sessions, stimulus_onsets, stimulus_durations, simulated_distrib)\n",
    "\n",
    "        ### LOOP OVER THE SCHEME\n",
    "        for k_fit_scheme in range(n_schemes):\n",
    "\n",
    "            # Current schemes\n",
    "            fit_scheme = scheme_array[k_fit_scheme]\n",
    "\n",
    "            # We replace the right value of the \"t\"'s according to the type of tuning curve and the N\n",
    "            if fit_scheme.find('gaussian') != -1:\n",
    "                fit_N = optimal_fit_N_array[k_fit_scheme]\n",
    "                fit_t_mu = optimal_t_mu_array[k_fit_scheme]\n",
    "                fit_t_conf = optimal_t_conf_array[k_fit_scheme]\n",
    "\n",
    "                fit_tc_type = 'gaussian'\n",
    "                # Creation of the true tuning curve objects\n",
    "                fit_tc_mu = tuning_curve(fit_tc_type, fit_N, fit_t_mu, tc_lower_bound_mu, tc_upper_bound_mu)\n",
    "                fit_tc_conf = tuning_curve(fit_tc_type, fit_N, fit_t_conf, tc_lower_bound_conf,\n",
    "                                             tc_upper_bound_conf)\n",
    "\n",
    "            elif fit_scheme.find('sigmoid') != -1:\n",
    "                fit_N = optimal_fit_N_array[k_fit_scheme]\n",
    "                fit_t_mu = optimal_t_mu_array[k_fit_scheme]\n",
    "                fit_t_conf = optimal_t_conf_array[k_fit_scheme]\n",
    "\n",
    "                fit_tc_type = 'sigmoid'\n",
    "                # Creation of the true tuning curve objects\n",
    "                fit_tc_mu = tuning_curve(fit_tc_type, fit_N, fit_t_mu, tc_lower_bound_mu, tc_upper_bound_mu)\n",
    "                fit_tc_conf = tuning_curve(fit_tc_type, fit_N, fit_t_conf, tc_lower_bound_conf,\n",
    "                                             tc_upper_bound_conf)\n",
    "\n",
    "            if fit_scheme.find('ppc') != -1:\n",
    "                fit_tc = [fit_tc_mu, fit_tc_conf]\n",
    "            elif fit_scheme.find('dpc') != -1:\n",
    "                fit_tc = [fit_tc_mu]\n",
    "            elif fit_scheme.find('rate') != -1:\n",
    "                fit_tc = []\n",
    "\n",
    "            # Regressor and BOLD computation\n",
    "            X[k_fit_scheme][k_subject][k_session] = simu_fmri.get_regressor(exp, fit_scheme, fit_tc)\n",
    "            \n",
    "            \n",
    "            # Rescale the regressors for rate code\n",
    "            if fit_scheme.find('rate') != -1:\n",
    "                X[k_fit_scheme][k_subject][k_session][:, 1] = mu_mean/conf_mean * X[k_fit_scheme][k_subject][k_session][:, 1]            \n",
    "\n",
    "    end = time.time()\n",
    "    print('Design matrix creation : Subject n'+str(k_subject)+' is done ! Time elapsed : '+str(end-start)+'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the data\n",
    "\n",
    "### Row data loading and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load the design matrices and specify their size\n",
    "with open(\"output/design_matrices/X_2_20sub.txt\", \"rb\") as fp:   # Unpickling\n",
    "    X = pickle.load(fp)\n",
    "\n",
    "# # Just for now\n",
    "# n_subjects = 1\n",
    "# n_sessions = 4\n",
    "# n_N = 3\n",
    "# n_schemes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot signal compared to summary statistics for the desired model\n",
    "######################################################################################\n",
    "k_fit_scheme = 4\n",
    "k_subject = 0\n",
    "k_session = 3\n",
    "n_stimuli = 380\n",
    "#######################################################################################\n",
    "# Tuning curve indices to visualize\n",
    "i_mu = 1\n",
    "i_conf = 0\n",
    "\n",
    "print(fit_N)\n",
    "\n",
    "# Test everything is fine\n",
    "mu = p1_mu_array[k_subject, k_session, :n_stimuli]\n",
    "sigma = p1_sd_array[k_subject, k_session, :n_stimuli]\n",
    "conf = -np.log(sigma)\n",
    "dist = p1_dist_array[k_subject, k_session, :, :n_stimuli]\n",
    "\n",
    "\n",
    "fit_scheme = scheme_array[k_fit_scheme]\n",
    "print('X of size : '+str(X[k_fit_scheme][k_subject][k_session].shape))\n",
    "data_signal_mu = copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, i_mu])\n",
    "print('Scheme : '+fit_scheme)\n",
    "print('Subject n°'+str(k_subject))\n",
    "print('Session n°'+str(k_session)+'\\n')\n",
    "\n",
    "n_scans = X[k_fit_scheme][k_subject][k_session].shape[0]\n",
    "\n",
    "\n",
    "if k_fit_scheme < 4:    # PPC or DPC\n",
    "    # For gaussian PPC\n",
    "    fit_N = int(optimal_fit_N_array[k_fit_scheme])\n",
    "\n",
    "    # For gaussian PPC\n",
    "    if fit_scheme.find('gaussian')!=-1:\n",
    "        tc_type = 'gaussian'\n",
    "    elif fit_scheme.find('sigmoid')!=-1:\n",
    "        tc_type = 'sigmoid'\n",
    "\n",
    "    # For mu\n",
    "    t_mu = optimal_t_mu_array[k_fit_scheme]\n",
    "\n",
    "    tc_mu = tuning_curve(tc_type, fit_N, t_mu, tc_lower_bound_mu, tc_upper_bound_mu)\n",
    "\n",
    "    tc_signal_mu =  tc_mu.f(mu, i_mu)\n",
    "\n",
    "    print('N='+str(fit_N))\n",
    "    print('Tuning curve n°'+str(i_mu)+' related to mu')\n",
    "\n",
    "    \n",
    "    # for color plots of all tuning curves\n",
    "    data_mu = np.zeros((fit_N, n_scans))\n",
    "    for idx_mu in range(fit_N):\n",
    "        data_mu[idx_mu, :] = copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, idx_mu])\n",
    "\n",
    "    # Only if we ask for PPC we display confidence plots\n",
    "    if fit_scheme.find('ppc')!=-1:\n",
    "        # For gaussian PPC\n",
    "        if fit_scheme.find('gaussian')!=-1:\n",
    "            tc_type = 'gaussian'\n",
    "        elif fit_scheme.find('sigmoid')!=-1:\n",
    "            tc_type = 'sigmoid'\n",
    "            \n",
    "        t_conf = optimal_t_conf_array[k_fit_scheme]\n",
    "        tc_conf = tuning_curve(tc_type, fit_N, t_conf, tc_lower_bound_conf, tc_upper_bound_conf)\n",
    "\n",
    "        tc_signal_conf =  tc_conf.f(conf, i_conf)\n",
    "        print('X of size : '+str(X[k_fit_scheme][k_subject][k_session].shape))\n",
    "        data_signal_conf = copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, fit_N+i_conf])\n",
    "\n",
    "        data_conf = np.zeros((fit_N, n_scans))\n",
    "        for idx_conf in range(fit_N):\n",
    "            data_conf[idx_conf, :] = copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, fit_N+idx_conf])\n",
    "\n",
    "        bold_max = np.max(X[k_fit_scheme][k_subject][k_session].flatten())\n",
    "        \n",
    "        print('Scheme : '+scheme_array[k_fit_scheme])\n",
    "        print('N='+str(fit_N))\n",
    "        print('Subject n°'+str(k_subject))\n",
    "        print('Session n°'+str(k_session)+'\\n')\n",
    "        print('Tuning curve n°'+str(i_conf)+' related to the confidence')\n",
    "else:\n",
    "    data_signal_mu = copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, 0])\n",
    "    data_signal_conf = copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, 1])\n",
    "\n",
    "### VISUALIZATION\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Mu sequence\n",
    "\n",
    "ax1_up = fig.add_subplot(521)\n",
    "ax1_up.plot(np.linspace(0, len(mu), len(mu)), mu, color='black')\n",
    "ax1_up.set_xlabel('Stimulus index')\n",
    "ax1_up.set_ylabel('Mu')\n",
    "ax1_up.set_title('Mu sequence')\n",
    "\n",
    "if k_fit_scheme < 4:    # PPC or DPC\n",
    "\n",
    "    # Tuning curves\n",
    "    ax1_down = fig.add_subplot(523)\n",
    "    mu = np.linspace(tc_lower_bound_mu, tc_upper_bound_mu, 1000)\n",
    "    tc_color = [None for k in range(fit_N)]\n",
    "    for k in range(fit_N):\n",
    "        p = ax1_down.plot(mu, tc_mu.f(mu, k))\n",
    "        tc_color[k] = p[0].get_color()\n",
    "    ax1_down.set_xlabel('Mu')\n",
    "    ax1_down.set_ylabel('Average firing rate')\n",
    "    ax1_down.set_title('Tuning curves for mu')\n",
    "\n",
    "    # Tuning curve value sequence\n",
    "    ax_up = fig.add_subplot(525)\n",
    "    ax_up.plot(np.linspace(0, n_stimuli, n_stimuli), tc_signal_mu, color=tc_color[i_mu])\n",
    "    ax_up.set_xlabel('Stimulus index')\n",
    "    ax_up.set_ylim(0, 1)\n",
    "    ax_up.set_ylabel('Tuning curve n°'+str(i_mu)+' value')\n",
    "    ax_up.set_title('Tuning curve n°'+str(i_mu)+' value sequence')\n",
    "\n",
    "    ax_down = fig.add_subplot(527)\n",
    "    between_scans_time = 2.0\n",
    "    ax_down.set_xlabel('Time (s)')\n",
    "    ax_down.set_ylabel('BOLD')\n",
    "    ax_down.set_title('Regressor BOLD sequence (mu)')\n",
    "    ax_down.set_ylim(0, bold_max)\n",
    "    ax_down.plot(np.linspace(0, between_scans_time*len(data_signal_mu), len(data_signal_mu)), data_signal_mu, color=tc_color[i_mu])\n",
    "\n",
    "    ax = fig.add_subplot(529)\n",
    "    heatmap = ax.imshow(data_mu, extent=[0, 5, 0, 1])\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    fontsize = 15\n",
    "    # # want a more natural, table-like display\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_ylabel('Tuning curve index', fontsize=fontsize)\n",
    "    ax.set_xlabel('Scans', fontsize=fontsize)\n",
    "    ax.set_title('Regressors for mu', y=1.08, fontsize=fontsize)\n",
    "\n",
    "# Only if we ask for PPC we display confidence plots\n",
    "if fit_scheme.find('ppc')!=-1:\n",
    "    # Confidence sequence\n",
    "    conf = -np.log(p1_sd_array[k_subject, k_session, :n_stimuli])\n",
    "    ax1__up = fig.add_subplot(522)\n",
    "    ax1__up.plot(np.linspace(0, len(conf), len(conf)), conf, color='black')\n",
    "    ax1__up.set_xlabel('Stimulus index')\n",
    "    ax1__up.set_ylabel('Confidence')\n",
    "    ax1__up.set_title('Confidence sequence')\n",
    "\n",
    "    # Tuning curves\n",
    "    ax1__down = fig.add_subplot(524)\n",
    "    conf = np.linspace(tc_lower_bound_conf, tc_upper_bound_conf, 1000)\n",
    "    tc_color = [None for k in range(fit_N)]\n",
    "    for k in range(fit_N):\n",
    "        p = ax1__down.plot(conf, tc_conf.f(conf, k))\n",
    "        tc_color[k] = p[0].get_color()\n",
    "    ax1__down.set_xlabel('Confidence')\n",
    "    ax1__down.set_ylabel('Average firing rate')\n",
    "    ax1__down.set_title('Tuning curves for confidence')\n",
    "\n",
    "    # Tuning curve value sequence\n",
    "    ax__up = fig.add_subplot(526)\n",
    "    ax__up.plot(np.linspace(0, n_stimuli, n_stimuli), tc_signal_conf, color=tc_color[i_conf])\n",
    "    ax__up.set_xlabel('Stimulus index')\n",
    "    ax__up.set_ylim(0, 1)\n",
    "    ax__up.set_ylabel('Tuning curve n°'+str(i_conf)+' value')\n",
    "    ax__up.set_title('Tuning curve n°'+str(i_conf)+' value sequence')\n",
    "\n",
    "    ax__down = fig.add_subplot(528)\n",
    "    between_scans_time = 2.0\n",
    "    ax__down.set_xlabel('Time (s)')\n",
    "    ax__down.set_ylabel('BOLD')\n",
    "    ax__down.set_title('Regressor BOLD sequence (conf)')\n",
    "    ax__down.set_ylim(0, bold_max)\n",
    "    ax__down.plot(np.linspace(0, between_scans_time*len(data_signal_conf), len(data_signal_conf)), data_signal_conf, color=tc_color[i_conf])\n",
    "    \n",
    "    ax = fig.add_subplot(5,2,10)\n",
    "    heatmap = ax.imshow(data_conf, extent=[0, 5, 0, 1])\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # # want a more natural, table-like display\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_ylabel('Tuning curve index', fontsize=fontsize)\n",
    "    ax.set_xlabel('Scans', fontsize=fontsize)\n",
    "    ax.set_title('Regressors for conf', y=1.08, fontsize=fontsize)\n",
    "\n",
    "    \n",
    "if fit_scheme.find('rate')!=-1:\n",
    "    \n",
    "    ax_down = fig.add_subplot(523)\n",
    "    between_scans_time = 2.0\n",
    "    ax_down.set_xlabel('Time (s)')\n",
    "    ax_down.set_ylabel('BOLD')\n",
    "    ax_down.set_title('Regressor BOLD sequence (mu)')\n",
    "    ax_down.plot(np.linspace(0, between_scans_time*len(data_signal_mu), len(data_signal_mu)), data_signal_mu)\n",
    "\n",
    "\n",
    "    # Confidence sequence\n",
    "    ax1__up = fig.add_subplot(522)\n",
    "    ax1__up.plot(np.linspace(0, len(conf), len(conf)), conf, color='black')\n",
    "    ax1__up.set_xlabel('Stimulus index')\n",
    "    ax1__up.set_ylabel('Confidence')\n",
    "    ax1__up.set_title('Confidence sequence')\n",
    "\n",
    "    ax__down = fig.add_subplot(524)\n",
    "    between_scans_time = 2.0\n",
    "    ax__down.set_xlabel('Time (s)')\n",
    "    ax__down.set_ylabel('BOLD')\n",
    "    ax__down.set_title('Regressor BOLD sequence (conf)')\n",
    "    ax__down.plot(np.linspace(0, between_scans_time*len(data_signal_conf), len(data_signal_conf)), data_signal_conf)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Future whitening visualization (with mu)\n",
    "X_before_whitening = copy.deepcopy(X)\n",
    "whitening_done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whiten the design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Whiten the design matrices\n",
    "\n",
    "# Whitening matrix\n",
    "white_mat = sio.loadmat('data/simu/whitening_matrix.mat')\n",
    "W = white_mat['W']\n",
    "# Complete the in-between session \"holes\"\n",
    "W[300:600, 300:600] = W[20:320, 20:320]\n",
    "\n",
    "if not whitening_done:\n",
    "    # Multiplying the zscored X with the whitening matrix\n",
    "    for k_fit_scheme, k_subject, k_session in itertools.product(range(n_schemes), range(n_subjects), range(n_sessions)):\n",
    "        X_tmp = copy.deepcopy(X[k_fit_scheme][k_subject][k_session])    # Just to lighten code\n",
    "        rows_dim, columns_dim = X_tmp.shape\n",
    "        X_tmp = np.matmul(W[:rows_dim, :rows_dim], X_tmp)\n",
    "        X[k_fit_scheme][k_subject][k_session] = copy.deepcopy(X_tmp)\n",
    "\n",
    "whitening_done = True\n",
    "\n",
    "X_after_whitening = copy.deepcopy(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computes the response vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Creation of y from X to save computational resources\n",
    "# Initialization of the response vectors\n",
    "y = [[[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fraction in range(n_fractions)]\n",
    "for k_true_scheme in range(n_schemes)]\n",
    "\n",
    "# Initialization of the weights\n",
    "weights = [[[None for k_subject in range(n_subjects)] for k_fraction in range(n_fractions)] \n",
    "            for k_true_scheme in range(n_schemes)]\n",
    "\n",
    "\n",
    "### LOOP OVER THE SCHEME\n",
    "for k_true_scheme in range(n_schemes):\n",
    "    true_scheme = scheme_array[k_true_scheme]\n",
    "    # We replace the right value of the \"t\"'s according to the type of tuning curve\n",
    "\n",
    "    if true_scheme.find('gaussian') != -1:\n",
    "        true_N = int(optimal_fit_N_array[k_true_scheme])\n",
    "        true_t_mu = optimal_t_mu_array[k_true_scheme]\n",
    "        true_t_conf = optimal_t_conf_array[k_true_scheme]\n",
    "        true_tc_type = 'gaussian'\n",
    "        # Creation of the true tuning curve objects\n",
    "        true_tc_mu = tuning_curve(true_tc_type, true_N, true_t_mu, tc_lower_bound_mu, tc_upper_bound_mu)\n",
    "        true_tc_conf = tuning_curve(true_tc_type, true_N, true_t_conf, tc_lower_bound_conf,\n",
    "                                     tc_upper_bound_conf)\n",
    "\n",
    "\n",
    "    elif true_scheme.find('sigmoid') != -1:\n",
    "        true_N = int(optimal_fit_N_array[k_true_scheme])\n",
    "        true_t_mu = optimal_t_mu_array[k_true_scheme]\n",
    "        true_t_conf = optimal_t_conf_array[k_true_scheme]\n",
    "        true_tc_type = 'sigmoid'\n",
    "        # Creation of the true tuning curve objects\n",
    "        true_tc_mu = tuning_curve(true_tc_type, true_N, true_t_mu, tc_lower_bound_mu, tc_upper_bound_mu)\n",
    "        true_tc_conf = tuning_curve(true_tc_type, true_N, true_t_conf, tc_lower_bound_conf,\n",
    "                                     tc_upper_bound_conf)\n",
    "\n",
    "\n",
    "    # We consider combinations of population fractions for PPC and rate codes\n",
    "    if true_scheme.find('ppc') != -1 or true_scheme.find('rate') != -1:\n",
    "        # The number of population fraction tested (related to W)\n",
    "        population_fraction_array = copy.deepcopy(np.array([[0.5, 0.5], [0.25, 0.75], [0, 1], [0.75, 0.25], [1, 0]]))\n",
    "    elif true_scheme.find('dpc') != -1:  # DPC case\n",
    "        population_fraction_array = copy.deepcopy(np.array([[1]]))\n",
    "    n_population_fractions = len(population_fraction_array)\n",
    "\n",
    "    if true_scheme.find('ppc') != -1:\n",
    "        true_tc = [true_tc_mu, true_tc_conf]\n",
    "    elif true_scheme.find('dpc') != -1:\n",
    "        true_tc = [true_tc_mu]\n",
    "    elif true_scheme.find('rate') != -1:\n",
    "        true_tc = []\n",
    "\n",
    "    ### LOOP OVER THE SUBJECTS\n",
    "    for k_subject in range(n_subjects):\n",
    "\n",
    "        ### LOOP OVER THE W's\n",
    "        # The number of subpopulation fractions acc. to the scheme\n",
    "        n_subpopulation_fractions = int(n_fractions / n_population_fractions)\n",
    "        fraction_counter = 0\n",
    "        for k_subpopulation_fraction in range(n_subpopulation_fractions):\n",
    "            for k_population_fraction, population_fraction in enumerate(population_fraction_array):\n",
    "                # The number of populations acc. to the scheme (2 for PPC and rate, 1 for DPC)\n",
    "                n_population = len(population_fraction)\n",
    "                if true_scheme.find('ppc') != -1 or true_scheme.find('dpc') != -1:\n",
    "                    # We consider one sparsity per remainder value of the counter divided by the number\n",
    "                    # of combinations to be tested\n",
    "                    \n",
    "                    subpopulation_sparsity_exp = sparsity_exp_array[fraction_counter % n_sparsity_exp]\n",
    "                    # Fraction of each neural subpopulation\n",
    "                    subpopulation_fraction = neural_proba.get_subpopulation_fraction(n_population, true_N,\n",
    "                                                                                     subpopulation_sparsity_exp)\n",
    "                elif true_scheme.find('rate') != -1:  # Rate case\n",
    "                    subpopulation_fraction = np.array([[1.0],[1.0]])\n",
    "                    \n",
    "\n",
    "                # Generate the data from the voxel\n",
    "                true_voxel = voxel(true_scheme, population_fraction, subpopulation_fraction, true_tc)\n",
    "                n_true_features = n_population * len(subpopulation_fraction[0])\n",
    "                weights_tmp = np.reshape(true_voxel.weights, (n_true_features,))\n",
    "\n",
    "                # Allocation of the weight tensor\n",
    "                weights[k_true_scheme][fraction_counter][k_subject] \\\n",
    "                    = copy.deepcopy(weights_tmp)\n",
    "\n",
    "                ### LOOP OVER THE SESSIONS : simulating the response\n",
    "                for k_session in range(n_sessions):\n",
    "                        # We use X to compute y order to save some computation time\n",
    "                        # Temporary variables to lighten the reading\n",
    "                        y[k_true_scheme][fraction_counter][k_subject][\n",
    "                                k_session] = copy.deepcopy(np.matmul(X[k_true_scheme][k_subject][k_session], weights_tmp))\n",
    "\n",
    "                fraction_counter += 1\n",
    "\n",
    "y_without_noise = copy.deepcopy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise injection in the responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the amplitude of the noise\n",
    "noise_sd = np.zeros(n_schemes)\n",
    "for k_true_scheme in range(n_schemes):\n",
    "    all_y = np.asarray(y[k_true_scheme]).flatten()  # Concatenation of all y grouped together for SNR computation\n",
    "    noise_sd[k_true_scheme] = np.sqrt(np.var(all_y[0]) * (1 / snr - 1))  # std of the added gaussian noise\n",
    "    del all_y    # Free memory\n",
    "print(noise_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Add the noise\n",
    "for k_true_scheme, k_fraction, k_subject, k_session in itertools.product(range(n_schemes), range(n_fractions), range(n_subjects), range(n_sessions)):\n",
    "    y[k_true_scheme][k_fraction][k_subject][k_session] = y[k_true_scheme][k_fraction][k_subject][k_session] + np.random.normal(0, noise_sd[k_true_scheme], len(y[k_true_scheme][k_fraction][k_subject][k_session]))\n",
    "        \n",
    "y_with_noise = copy.deepcopy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High pass filtering of the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create the filtering design matrices and filters out the response\n",
    "\n",
    "for k_true_scheme, k_fraction, k_subject, k_sessions in itertools.product(range(n_schemes), range(n_fractions), range(n_subjects), range(n_sessions)):    \n",
    "    y_tmp = copy.deepcopy(y[k_true_scheme][k_fraction][k_subject][k_session])\n",
    "    N = len(y_tmp)    # Resolution of the signal\n",
    "    K = 11    # Highest order of the filter\n",
    "    n_grid = np.linspace(0, N-1, N, endpoint=True)    # 1D grid over values\n",
    "    k_grid = np.linspace(2, K, K-1, endpoint=True)    # 1D grid over orders\n",
    "    X_filter = np.zeros((N, K-1))\n",
    "    for kk, k in enumerate(k_grid):\n",
    "        X_filter[:, kk] = np.sqrt(2/N) * np.cos(np.pi*(2*n_grid+1)/(2*N)*(k-1))\n",
    "    y_tmp = copy.deepcopy(y_tmp - np.matmul(np.matmul(X_filter, np.transpose(X_filter)), y_tmp))    # Regression\n",
    "    y[k_true_scheme][k_fraction][k_subject][k_session] = copy.deepcopy(y_tmp)\n",
    "\n",
    "y_after_filtering = copy.deepcopy(y)\n",
    "\n",
    "# # To visualize the matrix\n",
    "# k_true_scheme = 0\n",
    "# k_fraction = 18\n",
    "# k_subject = 0\n",
    "# k_session = 3\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.imshow(X_filter, cmap=plt.cm.ocean, extent=[2, K, N-1,0], aspect='auto')\n",
    "# plt.colorbar()\n",
    "# plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-scoring of design matrices and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Z-scoring of X and y\n",
    "\n",
    "scaling_factor_X = 0.01\n",
    "\n",
    "# Initialization\n",
    "Xz = [[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)]\n",
    "     for k_fit_scheme in range(n_schemes)]\n",
    "\n",
    "yz = [[[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fraction in range(n_fractions)]\n",
    "for k_true_scheme in range(n_schemes)]\n",
    "      \n",
    "yz_without_noise = [[[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fraction in range(n_fractions)]\n",
    "for k_true_scheme in range(n_schemes)]\n",
    "\n",
    "for k_fit_scheme, k_subject, k_session in itertools.product(range(n_schemes), range(n_subjects), range(n_sessions)):\n",
    "    Xz[k_fit_scheme][k_subject][k_session] = np.zeros_like(X[k_fit_scheme][k_subject][k_session])\n",
    "\n",
    "# Manual Z-scoring of regressors inside the session\n",
    "for k_fit_scheme,k_subject, k_session in itertools.product(range(n_schemes), range(n_subjects), range(n_sessions)):\n",
    "    n_fit_features = len(X[k_fit_scheme][k_subject][0][0])\n",
    "    for feature in range(n_fit_features):\n",
    "        X_mean = np.mean(X[k_fit_scheme][k_subject][k_session][:, feature], axis=0)\n",
    "        Xz[k_fit_scheme][k_subject][k_session][:, feature]\\\n",
    "            = (copy.deepcopy(X[k_fit_scheme][k_subject][k_session][:, feature]) - X_mean * np.ones_like(\n",
    "            X[k_fit_scheme][k_subject][k_session][:, feature])) / scaling_factor_X  # Centering + Scaling\n",
    "    # End of z-scoring\n",
    "        \n",
    "y_sd = np.zeros(n_schemes)\n",
    "y_without_noise_sd = np.zeros(n_schemes)\n",
    "\n",
    "for k_true_scheme in range(n_schemes):\n",
    "    y_sd[k_true_scheme] = np.std(np.asarray(y[k_true_scheme]).flatten()[0])\n",
    "    y_without_noise_sd[k_true_scheme] = np.std(np.asarray(y_without_noise[k_true_scheme]).flatten()[0])\n",
    "    for k_fraction, k_subject in itertools.product(range(n_fractions), range(n_subjects)):    \n",
    "        # Z-scoring of y\n",
    "        for k_session in range(n_sessions):\n",
    "            y_mean = np.mean(y[k_true_scheme][k_fraction][k_subject][k_session], axis=0)\n",
    "\n",
    "            yz[k_true_scheme][k_fraction][k_subject][k_session] = \\\n",
    "                (copy.deepcopy(y[k_true_scheme][k_fraction][k_subject][k_session] - y_mean)/y_sd[k_true_scheme])    # Centering+standardization\n",
    "            yz_without_noise[k_true_scheme][k_fraction][k_subject][k_session] = \\\n",
    "                (copy.deepcopy(y_without_noise[k_true_scheme][k_fraction][k_subject][k_session] - y_mean)/y_sd[k_true_scheme])    # Centering+standardization\n",
    "\n",
    "        ### End of z-scoring of y\n",
    "    \n",
    "# Reajusting the weights after zscoring\n",
    "for k_true_scheme,k_fraction, k_subject in itertools.product(range(n_schemes), range(n_fractions), range(n_subjects)):    \n",
    "    for feature in range(weights[k_true_scheme][k_fraction][k_subject].shape[0]):\n",
    "        weights[k_true_scheme][k_fraction][k_subject][feature] = weights[k_true_scheme][k_fraction][k_subject][feature]*scaling_factor_X/y_sd[k_true_scheme]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualization of the different processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# PICK HERE WHICH REGRESSOR AND RESPONSE TO VISUALIZE\n",
    "####################################################\n",
    "k_scheme = 0\n",
    "k_fraction = 1\n",
    "k_subject = 0\n",
    "k_session = 3\n",
    "i = 1   # Index of the regressor to visualize\n",
    "####################################################\n",
    "\n",
    "y1_plot = copy.deepcopy(y_without_noise[k_scheme][k_fraction][k_subject][k_session])\n",
    "y2_plot = copy.deepcopy(y_with_noise[k_scheme][k_fraction][k_subject][k_session])\n",
    "y3_plot = copy.deepcopy(y_after_filtering[k_scheme][k_fraction][k_subject][k_session])\n",
    "y4_plot = copy.deepcopy(yz[k_scheme][k_fraction][k_subject][k_session])\n",
    "\n",
    "x1_plot = copy.deepcopy(X_before_whitening[k_scheme][k_subject][k_session][:, i])\n",
    "x2_plot = copy.deepcopy(X_after_whitening[k_scheme][k_subject][k_session][:, i])\n",
    "x3_plot = copy.deepcopy(Xz[k_scheme][k_subject][k_session][:, i])\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "ax_up = fig.add_subplot(421)\n",
    "between_scans_time = 2.0\n",
    "ax_up.set_xlabel('Time (s)')\n",
    "ax_up.set_ylabel('BOLD')\n",
    "ax_up.set_title('Pure regressor')\n",
    "ax_up.plot(np.linspace(0, between_scans_time*len(x1_plot), len(x1_plot)), x1_plot)\n",
    "\n",
    "ax_down = fig.add_subplot(423)\n",
    "ax_down.set_xlabel('Time (s)')\n",
    "ax_down.set_ylabel('BOLD')\n",
    "ax_down.set_title('Regressor + whitening')\n",
    "ax_down.plot(np.linspace(0, between_scans_time*len(x2_plot), len(x2_plot)), x2_plot)\n",
    "\n",
    "ax_down = fig.add_subplot(425)\n",
    "ax_down.set_xlabel('Time (s)')\n",
    "ax_down.set_ylabel('BOLD')\n",
    "ax_down.set_title('Regressor + whitening + zscoring')\n",
    "ax_down.plot(np.linspace(0, between_scans_time*len(x3_plot), len(x3_plot)), x3_plot)\n",
    "\n",
    "ax1 = fig.add_subplot(422)\n",
    "ax1.plot(np.linspace(0, between_scans_time*len(y1_plot), len(y1_plot)), y1_plot)\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('BOLD')\n",
    "ax1.set_title('Pure response')\n",
    "\n",
    "ax2 = fig.add_subplot(424)\n",
    "ax2.plot(np.linspace(0, between_scans_time*len(y2_plot), len(y2_plot)), y2_plot)\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('BOLD')\n",
    "ax2.set_title('Response + Noise')\n",
    "\n",
    "ax3 = fig.add_subplot(426)\n",
    "ax3.plot(np.linspace(0, between_scans_time*len(y3_plot), len(y3_plot)), y3_plot)\n",
    "ax3.set_xlabel('Time (s)')\n",
    "ax3.set_ylabel('BOLD')\n",
    "ax3.set_title('Response + Noise + Filtering')\n",
    "\n",
    "ax4 = fig.add_subplot(428)\n",
    "ax4.plot(np.linspace(0, between_scans_time*len(y4_plot), len(y4_plot)), y4_plot)\n",
    "ax4.set_xlabel('Time (s)')\n",
    "ax4.set_ylabel('BOLD')\n",
    "ax4.set_title('Response + Noise + Filtering + Zscoring')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Visualization of an example of training and testing\n",
    "\n",
    "Here we select arbitrary values of hyperparameters and plot the result for one fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     70
    ]
   },
   "outputs": [],
   "source": [
    "# PICK HERE WHICH MODEL, SET, ETC. TO VISUALIZE\n",
    "#######################################\n",
    "k_fit_scheme = 0\n",
    "k_true_scheme = 0\n",
    "k_fraction = 10\n",
    "k_subject = 0\n",
    "k_session = 3    # Left-out session\n",
    "#######################################\n",
    "\n",
    "fit_scheme = scheme_array[k_fit_scheme]\n",
    "true_scheme = scheme_array[k_true_scheme]\n",
    "\n",
    "n_fit_features = len(X[k_fit_scheme][0][0][0, :])\n",
    "n_true_features = len(X[k_true_scheme][0][0][0, :])\n",
    "\n",
    "\n",
    "print('Fitted scheme : '+fit_scheme+'\\n'+'True scheme = '+str(true_scheme)+\n",
    "      '\\n'+'Subject n°'+str(k_subject)+'\\nLeft-out session : '+str(k_session))\n",
    "\n",
    "# Current cross-validation matrice and response\n",
    "X_cv = copy.deepcopy(Xz[k_fit_scheme][k_subject])\n",
    "y_cv = copy.deepcopy(yz[k_true_scheme][k_fraction][k_subject])\n",
    "\n",
    "y_without_noise_cv = copy.deepcopy(yz_without_noise[k_true_scheme][k_fraction][k_subject])\n",
    "\n",
    "X_train = copy.deepcopy(np.concatenate(X_cv[:k_session]+X_cv[k_session+1:], axis=0))\n",
    "y_train = copy.deepcopy(np.concatenate(y_cv[:k_session]+y_cv[k_session+1:], axis=0))\n",
    "y_without_noise_train = copy.deepcopy(np.concatenate(y_without_noise_cv[:k_session]+y_without_noise_cv[k_session+1:], axis=0))\n",
    "\n",
    "X_test = copy.deepcopy(X_cv[k_session])\n",
    "y_test = copy.deepcopy(y_cv[k_session])\n",
    "y_without_noise_test = copy.deepcopy(y_without_noise_cv[k_session])\n",
    "\n",
    "\n",
    "# Train the model using the training set\n",
    "regr.fit(X_train, y_train)\n",
    "y_hat_train = regr.predict(X_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = np.transpose(np.array([regr.predict(X_test)]))\n",
    "\n",
    "# Second fit\n",
    "regr2.fit(y_pred, y_test)\n",
    "y_pred2 = regr2.predict(y_pred)\n",
    "\n",
    "# Train and test results\n",
    "r2_raw_test_unique = r2_score(y_test, y_pred2)\n",
    "r2_raw_train_unique = r2_score(y_train, y_hat_train)\n",
    "rho_raw_train_unique = pearsonr(y_train, y_hat_train)[0]\n",
    "rho_raw_test_unique = pearsonr(y_test, y_pred2)[0]\n",
    "\n",
    "r2_true_test_unique = r2_score(y_without_noise_test, y_pred2)\n",
    "r2_true_train_unique = r2_score(y_without_noise_train, y_hat_train)\n",
    "rho_true_train_unique = pearsonr(y_without_noise_train, y_hat_train)[0]\n",
    "rho_true_test_unique = pearsonr(y_without_noise_test, y_pred2)[0]\n",
    "\n",
    "# Plot the signal and the response\n",
    "fig1 = plt.figure(figsize=(15,15), dpi= 80)\n",
    "ax_y_train = fig1.add_subplot(211)\n",
    "ax_y_train.plot(np.linspace(0, between_scans_time*len(y_train), len(y_train)), y_train, label='Data')\n",
    "ax_y_train.plot(np.linspace(0, between_scans_time*len(y_hat_train), len(y_hat_train)), y_hat_train, label='Fitted')\n",
    "ax_y_train.plot(np.linspace(0, between_scans_time*len(y_without_noise_train), len(y_without_noise_train)), y_without_noise_train, label='True')\n",
    "ax_y_train.legend()\n",
    "ax_y_train.set_xlabel('Time (s)')\n",
    "ax_y_train.set_ylabel('BOLD')\n",
    "ax_y_train.set_title('Training set fit')\n",
    "# Separate sessions\n",
    "#print(str(y_cv[0]))\n",
    "dash_line = [None for k in range(n_sessions-1)]\n",
    "dash_position = 0\n",
    "for k in range(n_sessions-1):\n",
    "    dash_position += between_scans_time*len(y_cv[k])\n",
    "    dash_line[k] = ax_y_train.plot(dash_position*np.ones(10), np.linspace(-1e-3, 1e-3, 10), color='black', linestyle='--')\n",
    "\n",
    "ax_y_test = fig1.add_subplot(212)\n",
    "ax_y_test.plot(np.linspace(0, between_scans_time*len(y_test), len(y_test)), y_test, label='Data')\n",
    "ax_y_test.plot(np.linspace(0, between_scans_time*len(y_pred), len(y_pred)), y_pred2, label='Predicted')\n",
    "ax_y_test.plot(np.linspace(0, between_scans_time*len(y_without_noise_test), len(y_without_noise_test)), y_without_noise_test, label='True')\n",
    "ax_y_test.legend()\n",
    "ax_y_test.set_xlabel('Time (s)')\n",
    "ax_y_test.set_ylabel('BOLD')\n",
    "ax_y_test.set_title('Test set predictions')\n",
    "fig1.tight_layout()\n",
    "\n",
    "print('r2_true_test = '+str(r2_true_test_unique))\n",
    "print('r2_raw_test = '+str(r2_raw_test_unique))\n",
    "print('rho_true_test = '+str(rho_true_test_unique))\n",
    "print('rho_raw_test = '+str(rho_raw_test_unique))\n",
    "\n",
    "## Plot the weights\n",
    "\n",
    "true_weights = weights[k_true_scheme][k_fraction][k_subject]\n",
    "fit_weights = regr.coef_\n",
    "\n",
    "# Plot the true weights\n",
    "\n",
    "width = 1/np.sqrt((n_true_features))\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,8))\n",
    "ax_true_weights = fig2.add_subplot(211)\n",
    "labels = [None for k in range(n_true_features)]\n",
    "for k in range(n_true_features):\n",
    "    if k<int(n_true_features/2):\n",
    "        labels[k] = 'a-TC n°'+str(k%int(n_true_features/2))\n",
    "    else:\n",
    "        labels[k] = 'b-TC n°'+str(k%int(n_true_features/2))\n",
    "ax_true_weights.bar(labels, true_weights, width=width)\n",
    "ax_true_weights.tick_params(labelsize=15)\n",
    "ax_true_weights.set_title('True weights', fontsize=20)\n",
    "#ax_true_weights.set_ylim([-1, 1])\n",
    "ax_true_weights.set_xticklabels(labels, rotation=45, fontsize=10)\n",
    "ax_true_weights.axhline(0, color=\"black\", linewidth=1)\n",
    "\n",
    "width = 1/np.sqrt((n_fit_features))\n",
    "ax_fit_weights = fig2.add_subplot(212)\n",
    "labels = [None for k in range(n_fit_features)]\n",
    "for k in range(n_fit_features):\n",
    "    if k<int(n_fit_features/2):\n",
    "        labels[k] = 'a-TC n°'+str(k%int(n_fit_features/2))\n",
    "    else:\n",
    "        labels[k] = 'b-TC n°'+str(k%int(n_fit_features/2))\n",
    "ax_fit_weights.bar(labels, fit_weights, width=width)\n",
    "ax_fit_weights.tick_params(labelsize=15)\n",
    "ax_fit_weights.set_title('Fitted weights', fontsize=20)\n",
    "# ax_fit_weights.set_ylim([-1, 1])\n",
    "ax_fit_weights.set_xticklabels(labels, rotation=45, fontsize=10)\n",
    "ax_fit_weights.axhline(0, color=\"black\", linewidth=1)\n",
    "fig2.tight_layout()\n",
    "plt.show()    \n",
    "\n",
    "print(true_weights)\n",
    "print(fit_weights)\n",
    "\n",
    "\n",
    "snr_eff = np.var(y_without_noise_test)/(np.var(y_without_noise_test)+np.var(y_test-y_without_noise_test))\n",
    "print('SNR_eff = '+str(snr_eff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation loops (simulation 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# The loops\n",
    "# The quantity to be computed during the cross validation\n",
    "r2_test = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "r2_train = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "rho_test = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "rho_train = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "\n",
    "r2_true_test = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "r2_true_train = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "rho_true_test = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "rho_true_train = np.zeros((n_schemes, n_schemes, n_fractions, n_subjects, n_sessions))\n",
    "\n",
    "### BEGINNING OF LOOPS OVER HYPERPARAMETERS\n",
    "for k_fit_scheme, k_true_scheme, k_fraction, k_subject in itertools.product(range(n_schemes), range(n_schemes), range(n_fractions), range(n_subjects)):\n",
    "    # Current cross-validation matrix and response\n",
    "    X_cv = copy.deepcopy(Xz[k_fit_scheme][k_subject])\n",
    "    y_cv = copy.deepcopy(yz[k_true_scheme][k_fraction][k_subject])\n",
    "    y_without_noise_cv = copy.deepcopy(yz_without_noise[k_true_scheme][k_fraction][k_subject])\n",
    "\n",
    "    # LOOP OVER SESSIONS (CV)\n",
    "    for k_session in range(n_sessions):\n",
    "        X_train = copy.deepcopy(np.concatenate(X_cv[:k_session]+X_cv[k_session+1:], axis=0))\n",
    "        y_train = copy.deepcopy(np.concatenate(y_cv[:k_session]+y_cv[k_session+1:], axis=0))\n",
    "        X_test = copy.deepcopy(X_cv[k_session])\n",
    "        y_test = copy.deepcopy(y_cv[k_session])\n",
    "\n",
    "        y_without_noise_train = copy.deepcopy(np.concatenate(y_without_noise_cv[:k_session]+y_without_noise_cv[k_session+1:], axis=0))\n",
    "        y_without_noise_test = copy.deepcopy(y_without_noise_cv[k_session])\n",
    "        \n",
    "        # Train the model using the training set\n",
    "        regr.fit(X_train, y_train)\n",
    "        y_hat_train = regr.predict(X_train)\n",
    "        # Make predictions using the testing set\n",
    "        y_pred = regr.predict(X_test)\n",
    "        y_pred_tmp = np.transpose(np.array([y_pred]))\n",
    "\n",
    "        # Second fit\n",
    "        regr2.fit(y_pred_tmp, y_test)\n",
    "        y_pred2 = regr2.predict(y_pred_tmp)\n",
    "\n",
    "        # Train and test results\n",
    "        r2_test_unique = r2_score(y_without_noise_test, y_pred2)\n",
    "        r2_train_unique = r2_score(y_train, y_hat_train)\n",
    "        rho_train_unique = pearsonr(y_train, y_hat_train)[0]\n",
    "        rho_test_unique = pearsonr(y_without_noise_test, y_pred2)[0]\n",
    "\n",
    "        r2_train[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = r2_score(y_train, y_hat_train)\n",
    "        r2_test[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = r2_score(y_test, y_pred2)\n",
    "        rho_train[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = pearsonr(y_train, y_hat_train)[0]\n",
    "        rho_test[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = pearsonr(y_test, y_pred2)[0]\n",
    "        rho_true_train[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = pearsonr(y_without_noise_train, y_hat_train)[0]\n",
    "        rho_true_test[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = pearsonr(y_without_noise_test, y_pred2)[0]\n",
    "\n",
    "        r2_true_train[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = r2_score(y_without_noise_train, y_hat_train)\n",
    "            \n",
    "        r2_true_test[k_fit_scheme, k_true_scheme, k_fraction, k_subject, k_session] \\\n",
    "            = r2_score(y_without_noise_test, y_pred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle the data\n",
    "\n",
    "### Row data loading and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some reloading (run that if the loops have already been performed and results saved)\n",
    "# r2_test = np.load('output/results/r2_test_snr'+str(snr)+'.npy')\n",
    "# r2_true_test = np.load('output/results/r2_true_test_snr'+str(snr)+'.npy')\n",
    "# r2_train = np.load('output/results/r2_train_snr'+str(snr)+'.npy')\n",
    "# rho_test = np.load('output/results/rho_test_snr'+str(snr)+'.npy')\n",
    "# rho_train = np.load('output/results/rho_train_snr'+str(snr)+'.npy')\n",
    "# r2_true_train = np.load('output/results/r2_true_train_snr'+str(snr)+'.npy')\n",
    "# rho_true_test = np.load('output/results/rho_true_test_snr'+str(snr)+'.npy')\n",
    "# rho_true_train = np.load('output/results/rho_true_train_snr'+str(snr)+'.npy')\n",
    "\n",
    "# for k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session in itertools.product(range(n_schemes), range(n_N)\n",
    "#                                                             , range(n_N), range(n_fractions), range(n_subjects), range(n_sessions)):\n",
    "#     if r2_true_test[k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session] < -3:\n",
    "#         print('k_scheme ='+str(k_scheme)+'\\nk_fit_N ='+str(k_fit_N)+'\\nk_true_N ='+str(k_true_N)+'\\nk_fraction ='+str(k_fraction)\n",
    "#              +'\\nk_subject = '+str(k_subject)+'\\nk_session = '+str(k_session))\n",
    "#         print('r2_test='+str(r2_test[k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session]))\n",
    "#         print('r2_true_test='+str(r2_true_test[k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session])+'\\n--------------')\n",
    "n_subjects = 2\n",
    "r2_raw_train_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "r2_raw_test_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "rho_raw_train_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "rho_raw_test_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "r2_true_train_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "r2_true_test_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "rho_true_train_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "rho_true_test_summary = np.zeros((n_schemes, n_schemes, n_fractions*n_subjects*n_sessions))\n",
    "\n",
    "# r2_raw_train_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# r2_raw_test_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# rho_raw_train_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# rho_raw_test_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# r2_true_train_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# r2_true_test_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# rho_true_train_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "# rho_true_test_summary = np.zeros((n_schemes, n_N, n_N, n_fractions*n_sessions))\n",
    "\n",
    "\n",
    "for k_fit_scheme, k_true_scheme in itertools.product(range(n_schemes), range(n_schemes)):\n",
    "    \n",
    "    r2_raw_test_summary[k_fit_scheme, k_true_scheme] = r2_test[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    r2_true_test_summary[k_fit_scheme, k_true_scheme]= r2_true_test[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    r2_raw_train_summary[k_fit_scheme, k_true_scheme] = r2_train[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    rho_raw_train_summary[k_fit_scheme, k_true_scheme] = rho_train[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    rho_raw_test_summary[k_fit_scheme, k_true_scheme] = rho_test[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    r2_true_train_summary[k_fit_scheme, k_true_scheme] = r2_true_train[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    rho_true_train_summary[k_fit_scheme, k_true_scheme] = rho_true_train[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "    rho_true_test_summary[k_fit_scheme, k_true_scheme] = rho_true_test[k_fit_scheme, k_true_scheme, :, :, :].flatten()\n",
    "\n",
    "#print(r2_raw_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Visualize results from simulation 1 (one example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize one specific plot\n",
    "\n",
    "#########################################\n",
    "k_fit_scheme = 1\n",
    "k_true_scheme = 1\n",
    "##########################################\n",
    "\n",
    "fit_scheme = scheme_array[k_fit_scheme]\n",
    "true_scheme = scheme_array[k_true_scheme]\n",
    "\n",
    "#r2\n",
    "metric_name = 'r2'\n",
    "true_or_raw = 'raw'\n",
    "train_or_test = 'test'\n",
    "exec(\"metric = %s_%s_%s_summary[k_fit_scheme, k_true_scheme]\" % (metric_name, true_or_raw, train_or_test))\n",
    "\n",
    "# # We set the negative r2 to the same value\n",
    "# negative_center = -0.2\n",
    "# n_negative_values = 0\n",
    "# if metric_name.find('r2')!=-1:\n",
    "#     for k, metric_value in enumerate(metric):\n",
    "#         if metric_value <0:\n",
    "#             metric[k] = negative_center\n",
    "#             n_negative_values += 1\n",
    "\n",
    "nbins = 20\n",
    "width = 0.03\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111) \n",
    "ax.hist(metric, bins=nbins, align='left', color='b', width=width)\n",
    "#ax.bar(negative_center, n_negative_values, color='r', width=2*width, align='center')\n",
    "ax.set_title(metric_name+' on '+true_or_raw+' '+train_or_test+'fit_scheme='+fit_scheme+', true_scheme='+true_scheme)\n",
    "ax.set_xlabel('{}_{}'.format(metric_name, train_or_test))\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlim([-0.3, 1.1])\n",
    "plt.show()\n",
    "\n",
    "print('r2 var: '+str(np.var(metric)))\n",
    "print('r2 mean: '+str(np.mean(metric)))\n",
    "\n",
    "# rho\n",
    "metric_name = 'rho'\n",
    "true_or_raw = 'raw'\n",
    "train_or_test = 'test'\n",
    "exec(\"metric = %s_%s_%s_summary[k_fit_scheme, k_true_scheme]\" % (metric_name, true_or_raw, train_or_test))\n",
    "\n",
    "nbins = 20\n",
    "width = 0.03\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111) \n",
    "ax.hist(metric, bins=nbins, align='left', color='b', width=width)\n",
    "ax.set_title(metric_name+' on '+true_or_raw+' '+train_or_test+'fit_scheme='+fit_scheme+', true_scheme='+true_scheme)\n",
    "ax.set_xlabel('{}_{}'.format(metric_name, train_or_test))\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlim([-1, 1.1])\n",
    "plt.show()\n",
    "\n",
    "print('rho var: '+str(np.var(metric)))\n",
    "print('rho mean: '+str(np.mean(metric)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot confusion matrices for simulation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "################################\n",
    "n_fractions = 20\n",
    "\n",
    "metric_name = 'r2'\n",
    "true_or_raw = 'true'\n",
    "train_or_test = 'test'\n",
    "\n",
    "print(metric_name+' on '+true_or_raw+' '+train_or_test+' set')\n",
    "\n",
    "# Visualize the entire confusion matrix\n",
    "nbins = 20\n",
    "width = 0.04\n",
    "\n",
    "#fig, ax = plt.subplots(n_N, n_N, sharex='col', sharey='row', figsize = (10, 10))\n",
    "#for k_fit_N in range(n_N):\n",
    "#    for k_true_N in range(n_N):\n",
    "#        exec(\"metric = %s_%s_%s_summary[k_scheme, k_fit_N, k_true_N]\" % (metric_name, true_or_raw, train_or_test))\n",
    "#        fit_N = N_array[k_fit_N]\n",
    "#        true_N = N_array[k_true_N]\n",
    "#        # We set the negative r2 to the same value\n",
    "#        negative_center = -0.2\n",
    "#        n_negative_values = 0\n",
    "#        if metric_name.find('r2')!=-1:\n",
    "#            for k, metric_value in enumerate(metric):\n",
    "#                if metric_value <0:\n",
    "#                    metric[k] = negative_center\n",
    "#                    n_negative_values += 1\n",
    "#\n",
    "#        ax[k_fit_N, k_true_N].hist(metric, bins=nbins, align='left', color='b', width=width)\n",
    "#        ax[k_fit_N, k_true_N].bar(negative_center, n_negative_values, color='r', width=2*width, align='center')\n",
    "#        ax[k_fit_N, k_true_N].set_title('N_fit='+str(fit_N)+', N_true='+str(true_N))\n",
    "#        ax[k_fit_N, k_true_N].set_xlabel('{}_{}'.format(metric_name, train_or_test))\n",
    "#        ax[k_fit_N, k_true_N].set_ylabel(\"Frequency\")\n",
    "#        if metric_name.find('r2')!=-1:\n",
    "#            ax[k_fit_N, k_true_N].set_xlim([-0.3, 1.1])\n",
    "#        else:\n",
    "#            ax[k_fit_N, k_true_N].set_xlim([-1, 1])\n",
    "#        ax[k_fit_N, k_true_N].set_ylim([0, n_fractions*n_subjects*n_sessions/4])\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Plot the confusion matrix of the mean\n",
    "column_labels = scheme_array\n",
    "row_labels = scheme_array\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "fontsize = 15\n",
    "exec(\"metric = copy.deepcopy(%s_%s_%s_summary)\" % (metric_name, true_or_raw, train_or_test))\n",
    "# We set the negative r2 to the same value\n",
    "data = np.mean(metric, axis=2)\n",
    "heatmap = ax.pcolor(data, cmap=plt.cm.Blues, vmin=np.min(data), vmax=np.max(data))\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_xticklabels(column_labels, minor=False, fontsize=fontsize)\n",
    "ax.set_yticklabels(row_labels, minor=False, fontsize=fontsize)\n",
    "ax.set_ylabel('Fitted scheme', fontsize=fontsize)\n",
    "ax.set_xlabel('True scheme', fontsize=fontsize)\n",
    "plt.title('Simulation 2, SNR='+str(snr), y=1.08, fontsize=20)\n",
    "cbar = fig.colorbar(heatmap, ticks=[np.min(data), np.max(data)])\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10,10))\n",
    "# ax = fig.add_subplot(111)\n",
    "# heatmap = ax.imshow(mean_over_X, extent=[0, 1, 0, 1])\n",
    "# cbar = fig.colorbar(heatmap, ticks=[np.min(mean_over_X), np.max(mean_over_X)])\n",
    "\n",
    "# # put the major ticks at the middle of each cell\n",
    "# ax.set_xticks([])\n",
    "# ax.set_yticks([])\n",
    "# ax.set_title('Regressor means')\n",
    "# ax.set_ylabel('Scheme')\n",
    "# ax.set_xlabel('true_N')\n",
    "# plt.show()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal number ofsubjects to be generated,\n",
    "\n",
    "Plot the mean and variance of these distributions for different number of subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Find the assymptotical value for n_fractions\n",
    "n_subjects = 1000\n",
    "n_subjects_array = np.linspace(1, n_subjects, num=int(n_subjects/5), endpoint=True).astype(int)\n",
    "\n",
    "subject_r2_train_mean = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "subject_r2_train_var = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "\n",
    "subject_r2_test_mean = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "subject_r2_test_var = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "\n",
    "subject_r2_true_test_mean = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "subject_r2_true_test_var = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "\n",
    "subject_rho_train_mean = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "subject_rho_train_var = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "\n",
    "subject_rho_test_mean = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "subject_rho_test_var = np.zeros((n_schemes, n_schemes, len(n_subjects_array)))\n",
    "\n",
    "for k_n_subjects, n_subjects in enumerate(n_subjects_array):\n",
    "    for k_fit_scheme, k_true_scheme in itertools.product(range(n_schemes), range(n_schemes)):            \n",
    "        subject_r2_train_mean[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.mean(r2_train[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "        subject_r2_train_var[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.var(r2_train[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "                                      \n",
    "        subject_r2_test_mean[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.mean(r2_test[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "        subject_r2_test_var[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.var(r2_test[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "        \n",
    "        subject_r2_true_test_mean[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.mean(r2_true_test[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "        subject_r2_true_test_var[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.var(r2_true_test[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "\n",
    "        subject_rho_train_mean[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.mean(rho_train[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "        subject_rho_train_var[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.var(rho_train[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "\n",
    "        subject_rho_test_mean[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.mean(rho_test[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())\n",
    "        subject_rho_test_var[k_fit_scheme, k_true_scheme, :] = \\\n",
    "            np.var(rho_test[k_fit_scheme, k_true_scheme, :, :n_subjects, :].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the evolution of r2 according to the number of subjects\n",
    "\n",
    "subject_r2_train_mean_to_plot = subject_r2_train_mean[k_fit_scheme, k_true_scheme, :]\n",
    "subject_r2_train_var_to_plot = subject_r2_train_var[k_fit_scheme, k_true_scheme, :]\n",
    "\n",
    "subject_r2_test_mean_to_plot = subject_r2_test_mean[k_fit_scheme, k_true_scheme, :]\n",
    "subject_r2_test_var_to_plot = subject_r2_test_var[k_fit_scheme, k_true_scheme, :]\n",
    "\n",
    "subject_rho_train_mean_to_plot = subject_rho_train_mean[k_fit_scheme, k_true_scheme, :]\n",
    "subject_rho_train_var_to_plot = subject_rho_train_var[k_fit_scheme, k_true_scheme, :]\n",
    "\n",
    "subject_rho_test_mean_to_plot = subject_rho_test_mean[k_fit_scheme, k_true_scheme, :]\n",
    "subject_rho_test_var_to_plot = subject_rho_test_var[k_fit_scheme, k_true_scheme, :]\n",
    "\n",
    "subject_r2_true_test_mean_to_plot = subject_r2_true_test_mean[k_fit_scheme, k_true_scheme, :]\n",
    "subject_r2_true_test_var_to_plot = subject_r2_true_test_var[k_fit_scheme, k_true_scheme, :]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax_r2_train_mean = fig.add_subplot(521)\n",
    "ax_r2_train_mean.plot(n_subjects_array, subject_r2_train_mean_to_plot)\n",
    "ax_r2_train_mean.set_xlabel('n_subjects')\n",
    "ax_r2_train_mean.set_ylabel('Mean of r2_train')\n",
    "ax_r2_train_mean.set_title('Mean of r2_train')\n",
    "\n",
    "ax_r2_train_var = fig.add_subplot(522)\n",
    "ax_r2_train_var.plot(n_subjects_array, subject_r2_train_var_to_plot)\n",
    "ax_r2_train_var.set_xlabel('n_subjects')\n",
    "ax_r2_train_var.set_ylabel('Variance of r2_train')\n",
    "ax_r2_train_var.set_title('Variance of r2_train')\n",
    "\n",
    "ax_r2_test_mean = fig.add_subplot(523)\n",
    "ax_r2_test_mean.plot(n_subjects_array, subject_r2_test_mean_to_plot)\n",
    "ax_r2_test_mean.set_xlabel('n_subjects')\n",
    "ax_r2_test_mean.set_ylabel('Mean of r2_test')\n",
    "ax_r2_test_mean.set_title('Mean of r2_test')\n",
    "\n",
    "ax_r2_test_var = fig.add_subplot(524)\n",
    "ax_r2_test_var.plot(n_subjects_array, subject_r2_test_var_to_plot)\n",
    "ax_r2_test_var.set_xlabel('n_subjects')\n",
    "ax_r2_test_var.set_ylabel('Variance of r2_test')\n",
    "ax_r2_test_var.set_title('Variance of r2_test')\n",
    "\n",
    "ax_rho_train_mean = fig.add_subplot(525)\n",
    "ax_rho_train_mean.plot(n_subjects_array, subject_rho_train_mean_to_plot)\n",
    "ax_rho_train_mean.set_xlabel('n_subjects')\n",
    "ax_rho_train_mean.set_ylabel('Mean of rho_train')\n",
    "ax_rho_train_mean.set_title('Mean of rho_train')\n",
    "\n",
    "ax_rho_train_var = fig.add_subplot(526)\n",
    "ax_rho_train_var.plot(n_subjects_array, subject_rho_train_var_to_plot)\n",
    "ax_rho_train_var.set_xlabel('n_subjects')\n",
    "ax_rho_train_var.set_ylabel('Variance of rho_train')\n",
    "ax_rho_train_var.set_title('Variance of rho_train')\n",
    "\n",
    "ax_rho_test_mean = fig.add_subplot(527)\n",
    "ax_rho_test_mean.plot(n_subjects_array, subject_rho_test_mean_to_plot)\n",
    "ax_rho_test_mean.set_xlabel('n_subjects')\n",
    "ax_rho_test_mean.set_ylabel('Mean of rho_test')\n",
    "ax_rho_test_mean.set_title('Mean of rho_test')\n",
    "\n",
    "ax_rho_test_var = fig.add_subplot(528)\n",
    "ax_rho_test_var.plot(n_subjects_array, subject_rho_test_var_to_plot)\n",
    "ax_rho_test_var.set_xlabel('n_subjects')\n",
    "ax_rho_test_var.set_ylabel('Variance of rho_test')\n",
    "ax_rho_test_var.set_title('Variance of rho_test')\n",
    "fig.tight_layout()\n",
    "\n",
    "ax_r2_true_test_mean = fig.add_subplot(5,2, 9)\n",
    "ax_r2_true_test_mean.plot(n_subjects_array, subject_r2_true_test_mean_to_plot)\n",
    "ax_r2_true_test_mean.set_xlabel('n_subjects')\n",
    "ax_r2_true_test_mean.set_ylabel('Mean of r2_true_test')\n",
    "ax_r2_true_test_mean.set_title('Mean of r2_true_test')\n",
    "\n",
    "ax_r2_true_test_var = fig.add_subplot(5,2,10)\n",
    "ax_r2_true_test_var.plot(n_subjects_array, subject_r2_true_test_var_to_plot)\n",
    "ax_r2_true_test_var.set_xlabel('n_subjects')\n",
    "ax_r2_true_test_var.set_ylabel('Variance of r2_true_test')\n",
    "ax_r2_true_test_var.set_title('Variance of r2_true_test')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize worse r2\n",
    "\n",
    "for k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session in itertools.product(range(n_schemes), \n",
    "                                                                            range(n_N), range(n_N), range(n_fractions), range(n_subjects), range(n_sessions)):\n",
    "    if r2_true_test[k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session] < -100:\n",
    "        print('k_scheme : '+str(k_scheme)+'\\n'+'k_fit_N = '+str(k_fit_N)+'\\nk_true_N = '+str(k_true_N)+'\\nk_fraction = '+str(k_fraction)\n",
    "          +'\\n'+'Subject n°'+str(k_subject)+'\\nLeft-out session : '+str(k_session))\n",
    "        print(r2_true_test[k_scheme, k_fit_N, k_true_N, k_fraction, k_subject, k_session])\n",
    "        print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
