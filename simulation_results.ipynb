{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of one voxel encoding for different probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import random as rand\n",
    "from scipy import io as sio\n",
    "from scipy import stats\n",
    "from scipy.stats.stats import pearsonr\n",
    "import numpy as np\n",
    "#import decimal\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')    # To avoid bugs\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import neural_proba\n",
    "from neural_proba import distrib\n",
    "from neural_proba import tuning_curve\n",
    "from neural_proba import voxel\n",
    "from neural_proba import experiment\n",
    "from neural_proba import fmri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the models\n",
    "\n",
    "Here are the properties related to the tuning curves (number, variance, ...) both true and fitted, to the neural mixture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seed to reproduce results from random processes\n",
    "rand.seed(5);\n",
    "\n",
    "# INPUTS\n",
    "\n",
    "# The parameters related to the scheme\n",
    "scheme_array = ['gaussian_ppc', 'sigmoid_ppc', 'gaussian_dpc', 'sigmoid_dpc']\n",
    "n_schemes = len(scheme_array)\n",
    "\n",
    "# The parameters related to the tuning curves to be explored\n",
    "#N_array = np.array([2, 4, 6, 8, 10, 14, 20])\n",
    "N_array = np.array([6, 8, 10, 14, 16, 20])\n",
    "\n",
    "t_mu_gaussian_array = np.array([0.15, 0.1, 7e-2, 5e-2, 4e-2, 3e-2, 2e-2])\n",
    "t_conf_gaussian_array = np.array([0.25, 0.15, 0.10, 8e-2, 6e-2, 4e-2, 3e-2])\n",
    "\n",
    "t_mu_sigmoid_array = np.sqrt(2*np.pi)/4*t_mu_gaussian_array\n",
    "t_conf_sigmoid_array = np.sqrt(2*np.pi)/4*t_conf_gaussian_array\n",
    "\n",
    "# Lower and upper bounds of the encoded summary quantity (for tuning curves)\n",
    "tc_lower_bound_mu = 0\n",
    "tc_upper_bound_mu = 1\n",
    "tc_lower_bound_conf = 1.1\n",
    "# we define the upper bound to be a bit away from the highest uncertainty\n",
    "tc_upper_bound_conf = 2.6\n",
    "\n",
    "# The number of N to be tested\n",
    "n_N = len(N_array)\n",
    "\n",
    "# The number of fractions tested (related to W)\n",
    "n_fractions = 20\n",
    "# Sparsity exponents\n",
    "sparsity_exp_array = np.array([1, 2, 4, 8])\n",
    "n_sparsity_exp = len(sparsity_exp_array)\n",
    "\n",
    "# The number of subjects\n",
    "n_subjects = 20\n",
    "\n",
    "# The number of sessions\n",
    "n_sessions = 4\n",
    "\n",
    "# The number of stimuli per session\n",
    "n_stimuli = 380\n",
    "\n",
    "# Way to compute the distributions from the sequence\n",
    "distrib_type = 'HMM'\n",
    "\n",
    "# Load the corresponding data\n",
    "[p1g2_dist_array, p1g2_mu_array, p1g2_sd_array] = neural_proba.import_distrib_param(n_subjects, n_sessions, n_stimuli,\n",
    "                                                                                      distrib_type)\n",
    "# SNR as defined by ||signal||²/(||signal||²+||noise||²)\n",
    "snr = 0.1\n",
    "\n",
    "# Type of regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computes the response vector list\n",
    "\n",
    "We load the design matrix, z-score it and get the response vector value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the design matrices\n",
    "with open(\"output/design_matrices/X_20sub.txt\", \"rb\") as fp:   # Unpickling\n",
    "    X = pickle.load(fp)\n",
    "\n",
    "# Z-SCORING\n",
    "\n",
    "# Initialization\n",
    "Xz = [[[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fit_N in range(n_N)]\n",
    "     for k_fit_scheme in range(n_schemes)]\n",
    "\n",
    "for k_fit_scheme, k_fit_N, k_subject, k_session in itertools.product(range(n_schemes), range(n_N), range(n_subjects), range(n_sessions)):\n",
    "    Xz[k_fit_scheme][k_fit_N][k_subject][k_session] = np.zeros_like(X[k_fit_scheme][k_fit_N][k_subject][k_session])\n",
    "\n",
    "# Manual Z-scoring of regressors inside the session\n",
    "for k_fit_scheme, k_fit_N, k_subject in itertools.product(range(n_schemes), range(n_N), range(n_subjects)):\n",
    "    n_fit_features = len(X[k_fit_scheme][k_fit_N][k_subject][0][0])\n",
    "    X_mean = np.mean(np.concatenate(X[k_fit_scheme][k_fit_N][k_subject], axis=0), axis=0)\n",
    "    X_sd = np.std(np.concatenate(X[k_fit_scheme][k_fit_N][k_subject], axis=0), axis=0)\n",
    "\n",
    "    for k_session in range(n_sessions):\n",
    "        for feature in range(n_fit_features):\n",
    "            Xz[k_fit_scheme][k_fit_N][k_subject][k_session][:, feature]\\\n",
    "                = (copy.deepcopy(X[k_fit_scheme][k_fit_N][k_subject][k_session][:, feature]) - X_mean[feature] * np.ones_like(\n",
    "                X[k_fit_scheme][k_fit_N][k_subject][k_session][:, feature])) / X_sd[feature]  # Centering + Standardization\n",
    "    # End of z-scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of y from X, first without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the response vectors and their zscore versions\n",
    "y = [[[[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fraction in range(n_fractions)]\n",
    "for k_true_N in range(n_N)] for k_true_scheme in range(n_schemes)]\n",
    "\n",
    "yz = [[[[[None for k_session in range(n_sessions)] for k_subject in range(n_subjects)] for k_fraction in range(n_fractions)]\n",
    "for k_true_N in range(n_N)] for k_true_scheme in range(n_schemes)]\n",
    "\n",
    "# Initialization of the weights\n",
    "\n",
    "weights = [[[None for k_fraction in range(n_fractions)] for k_true_N in range(n_N)]\n",
    "           for k_true_scheme in range(n_schemes)]\n",
    "\n",
    "\n",
    "### LOOP OVER THE SCHEME\n",
    "for k_true_scheme in range(n_schemes):\n",
    "    true_scheme = scheme_array[k_true_scheme]\n",
    "\n",
    "    # We replace the right value of the \"t\"'s according to the type of tuning curve\n",
    "\n",
    "    if true_scheme.find('gaussian') != -1:\n",
    "        true_t_mu_array = copy.deepcopy(t_mu_gaussian_array)\n",
    "        true_t_conf_array = copy.deepcopy(t_conf_gaussian_array)\n",
    "        true_tc_type = 'gaussian'\n",
    "\n",
    "    elif true_scheme.find('sigmoid') != -1:\n",
    "        true_t_mu_array = copy.deepcopy(t_mu_sigmoid_array)\n",
    "        true_t_conf_array = copy.deepcopy(t_conf_sigmoid_array)\n",
    "        true_tc_type = 'sigmoid'\n",
    "\n",
    "    # We consider combinations of population fractions for PPC and rate codes\n",
    "    if true_scheme.find('ppc') != -1 or true_scheme.find('rate') != -1:\n",
    "        # The number of population fraction tested (related to W)\n",
    "        population_fraction_array = copy.deepcopy(np.array([[0.5, 0.5], [0.25, 0.75], [0, 1], [0.75, 0.25], [1, 0]]))\n",
    "    elif true_scheme.find('dpc') != -1:  # DPC case\n",
    "        population_fraction_array = copy.deepcopy(np.array([[1]]))\n",
    "    n_population_fractions = len(population_fraction_array)\n",
    "\n",
    "    ### LOOP OVER N_true\n",
    "    for k_true_N in range(n_N):\n",
    "        true_N = N_array[k_true_N]\n",
    "        # Creation of the true tuning curve objects\n",
    "        true_t_mu = true_t_mu_array[k_true_N]\n",
    "        true_t_conf = true_t_conf_array[k_true_N]\n",
    "        true_tc_mu = tuning_curve(true_tc_type, true_N, true_t_mu, tc_lower_bound_mu, tc_upper_bound_mu)\n",
    "        true_tc_conf = tuning_curve(true_tc_type, true_N, true_t_conf, tc_lower_bound_conf,\n",
    "                                     tc_upper_bound_conf)\n",
    "\n",
    "        if true_scheme.find('ppc') != -1:\n",
    "            true_tc = [true_tc_mu, true_tc_conf]\n",
    "        elif true_scheme.find('dpc') != -1:\n",
    "            true_tc = [true_tc_mu]\n",
    "        elif true_scheme.find('rate') != -1:\n",
    "            true_tc = []\n",
    "\n",
    "        ### LOOP OVER THE W's\n",
    "        # The number of subpopulation fractions acc. to the scheme\n",
    "        n_subpopulation_fractions = int(n_fractions / n_population_fractions)\n",
    "        fraction_counter = 0\n",
    "        for k_population_fraction, population_fraction in enumerate(population_fraction_array):\n",
    "            # The number of populations acc. to the scheme (2 for PPC and rate, 1 for DPC)\n",
    "            n_population = len(population_fraction)\n",
    "            for k_subpopulation_fraction in range(n_subpopulation_fractions):\n",
    "                if true_scheme.find('ppc') != -1 or true_scheme.find('dpc') != -1:\n",
    "                    # We consider one sparsity per remainder value of the counter divided by the number\n",
    "                    # of combinations to be tested\n",
    "                    subpopulation_sparsity_exp = sparsity_exp_array[fraction_counter % n_sparsity_exp]\n",
    "                    # Fraction of each neural subpopulation\n",
    "                    subpopulation_fraction = neural_proba.get_subpopulation_fraction(n_population, true_N,\n",
    "                                                                                     subpopulation_sparsity_exp)\n",
    "                else:  # Rate case\n",
    "                    population_fraction = np.array([1, 1])\n",
    "\n",
    "                # Generate the data from the voxel\n",
    "                true_voxel = voxel(true_scheme, population_fraction, subpopulation_fraction,\n",
    "                                   [true_tc_mu, true_tc_conf])\n",
    "                n_true_features = n_population * true_N\n",
    "                weights_tmp = np.reshape(true_voxel.weights, (n_true_features,))\n",
    "                # Allocation of the weight tensor\n",
    "                weights[k_true_scheme][k_true_N][fraction_counter] \\\n",
    "                    = copy.deepcopy(weights_tmp)\n",
    "\n",
    "\n",
    "                ### LOOP OVER THE SUBJECTS\n",
    "                for k_subject in range(n_subjects):\n",
    "\n",
    "                    ### LOOP OVER THE SESSIONS : simulating the response\n",
    "                    for k_session in range(n_sessions):\n",
    "                            # We use X to compute y order to save some computation time\n",
    "                            # Temporary variables to lighten the reading\n",
    "                            X_tmp = copy.deepcopy(X[k_true_scheme][k_true_N][k_subject][k_session])\n",
    "                            y_tmp = copy.deepcopy(np.dot(X_tmp, weights_tmp))\n",
    "\n",
    "                            # Allocation of the tensor\n",
    "                            y[k_true_scheme][k_true_N][fraction_counter][k_subject][\n",
    "                                k_session] = copy.deepcopy(y_tmp)\n",
    "\n",
    "\n",
    "                fraction_counter += 1\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise injection\n",
    "\n",
    "#print('Without added noise :'+str(y[0][1][1][2][2]))\n",
    "\n",
    "# Compute the amplitude of the noise (std of the added gaussian noise)\n",
    "for k_fit_scheme, k_true_N in itertools.product(range(n_schemes), range(n_N)):\n",
    "    all_y = np.asarray(y[k_fit_scheme][k_true_N]).flatten()    # Concatenation of all y grouped together for SNR computation\n",
    "    print(np.sqrt(np.var(all_y)*(1/snr-1)))\n",
    "    noise_sd = np.sqrt(np.var(all_y)*(1/snr-1))\n",
    "    del all_y    # Free memory\n",
    "    for fraction_counter, k_subject, k_session in itertools.product(range(n_fractions), range(n_subjects), range(n_sessions)):\n",
    "        y[k_true_scheme][k_true_N][fraction_counter][k_subject][k_session] = copy.deepcopy(y[k_true_scheme][k_true_N][fraction_counter][k_subject][k_session] \n",
    "            + np.random.normal(0, noise_sd, len(y[k_true_scheme][k_true_N][fraction_counter][k_subject][k_session])))\n",
    "        print(np.random.normal(0, noise_sd, len(y[k_true_scheme][k_true_N][fraction_counter][k_subject][k_session])))\n",
    "\n",
    "# PB : MEME Y AVANT ET APRES LE BRUIT ! ! ! ! \n",
    "#print('With added noise :'+str(y[0][1][1][2][2]))\n",
    "\n",
    "for k_fit_scheme, k_true_N, fraction_counter, k_subject, k_session in itertools.product(range(n_schemes), range(n_N), range(n_fractions), range(n_subjects), range(n_sessions)):\n",
    "\n",
    "    # Z-scoring of y\n",
    "    y_mean = np.mean(np.concatenate(np.asarray(y[k_true_scheme][fraction_counter][k_true_N][k_subject]),\n",
    "                                    axis=0))\n",
    "    y_sd = np.std(np.concatenate(np.asarray(y[k_true_scheme][fraction_counter][k_true_N][k_subject]),\n",
    "                                 axis=0))\n",
    "\n",
    "    for k_session in range(n_sessions):\n",
    "        yz[k_true_scheme][fraction_counter][k_true_N][k_subject][k_session] = \\\n",
    "            (copy.deepcopy(y[k_true_scheme][fraction_counter][k_true_N][k_subject][k_session]) - y_mean) / y_sd    # Centering and standardization\n",
    "\n",
    "    ### End of z-scoring of y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of training and test set : illustration, example\n",
    "\n",
    "Here we select arbitrary values of hyperparameters and plot the result for one fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scheme = 0\n",
    "k_true_N = 1\n",
    "k_fraction = 10\n",
    "k_subject = 0\n",
    "\n",
    "print('Scheme :'+scheme_array[k_scheme]+'\\n'+'true_N ='+str(N_array[k_true_N])+'\\n'+'Subject number '+str(k_subject))\n",
    "\n",
    "# width = np.mean(mu)/(2*ppc_voxel.tuning_curve[0].N)\n",
    "#\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# x = np.linspace(0, 1, ppc_voxel.tuning_curve[0].N)\n",
    "# ax.bar(x, ppc_voxel.subpopulation_fraction[0], width=width)\n",
    "# utils.plot_detail(fig, ax, 'p(Head)', 'Neural fraction', xtick_fontsize, xfontstyle='italic')\n",
    "# ax.tick_params(labelsize=xtick_fontsize)\n",
    "# ax.set_title('Mixture in this voxel', fontsize=20)\n",
    "# plt.savefig('output/figures/mixture_mu.png', bbox_inches='tight')\n",
    "#\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "# width = np.mean(mu)/(2*ppc_voxel.tuning_curve[1].N)*(tc_upper_bound_conf-tc_lower_bound_conf)\n",
    "#\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# x = np.linspace(tc_lower_bound_conf, tc_upper_bound_conf, ppc_voxel.tuning_curve[1].N)\n",
    "# ax.bar(x, ppc_voxel.subpopulation_fraction[1], width=width)\n",
    "# utils.plot_detail(fig, ax, 'Uncertainty (s.d.)', 'Neural fraction', xtick_fontsize)\n",
    "# ax.set_title('Mixture in this voxel', fontsize=20)\n",
    "# plt.savefig('output/figures/mixture_conf.png', bbox_inches='tight')\n",
    "#\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The quantity to be computed during the cross validation\n",
    "r2_test = np.zeros((n_schemes, n_N, n_N, n_fractions, n_subjects, n_sessions))\n",
    "r2_train = np.zeros((n_schemes, n_N, n_N, n_fractions, n_subjects, n_sessions))\n",
    "rho_test = np.zeros((n_schemes, n_N, n_N, n_fractions, n_subjects, n_sessions))\n",
    "rho_train = np.zeros((n_schemes, n_N, n_N, n_fractions, n_subjects, n_sessions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "a = [[[1, 100], [10,2], [40, 3]], [[10, 1], [1,21], [1, 32]], [[90, 1], [0.1,2], [0, 3]], [[1, 1], [1,2], [1, 3]]]\n",
    "b = np.concatenate(np.concatenate(a, axis=0), axis=0)\n",
    "c = np.asarray(a).flatten()\n",
    "print(np.sqrt(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
